{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSM Assignment 3\n",
    "Please pay attention to the following instructions:\n",
    "1. Please follow carefully the steps outlined in the assignment. If you cannot solve an exercise and this hinders continuing with subsequent exercises, try to find a way to work around it and give a clear explanation for the solution you have chosen.\n",
    "2. Submit your work in the form of a Jupyter notebook via Canvas, before the deadline. Your notebook should not give errors when executed with `Run All`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: importing the relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** import all the libraries you are using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from logistic_regression_src import ClassifierBase, plot_digits_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: binary logistic regression (only examples, no exercises)\n",
    "In this section, we will train a binary logistic regression model based on a simplified version of the MNIST dataset. In the cell below, this dataset is imported and visualized. It consists of 360 images (resolution 8x8), of which 178 are handwritten zeros and the other 182 are handwritten ones. The images are stored in `X`, while the targets (a 1 or 0) are stored in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations: 360\n",
      "Number of observations of class 0: 178\n",
      "Number of observations of class 1: 182\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL!!! #\n",
    "digits = datasets.load_digits()\n",
    "X = digits.images[np.logical_or(digits.target == 0, digits.target == 1)]\n",
    "y = digits.target[np.logical_or(digits.target == 0, digits.target == 1)].reshape(-1, 1)\n",
    "N = len(y)\n",
    "K = np.unique(y).shape[0]\n",
    "digits.target_names = np.unique(y).sort()\n",
    "\n",
    "print(f\"Total number of observations: {N}\")\n",
    "for k in range(K):\n",
    "    print(f\"Number of observations of class {k}: {np.sum(y == k)}\")\n",
    "\n",
    "plot_digits_sample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** the goal of this example is to train a logistic regression model for the binary classification task of the simplified MNIST dataset above. Each image can be considered as a point in a 8x8=64 dimensional feature space. The target variable is binary: 0 or 1.\n",
    "\n",
    "Write a class `BinaryLogisticRegressionClassifier`, with parental class `ClassifierBase` that does this. Use stochastic gradient descent for training and use as the loss function the negative log likelihood. At initialization, set `learning_rate`, `batch_size` and `epochs` to default values 0.0001, 20 and 3, respectively.\n",
    "\n",
    "Include in the `fit` method a `verbose` keyword. If `True`, after each epoch the training error should be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionClassifier(ClassifierBase):\n",
    "    \"\"\"\n",
    "    Binary logistic regression classifier using stochastic gradient descent.\n",
    "\n",
    "    This class implements a binary logistic regression model for classification tasks,\n",
    "    using negative log likelihood as the loss function and SGD for optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default=0.0001\n",
    "        Learning rate for gradient descent.\n",
    "    batch_size : int, default=20\n",
    "        Number of samples per batch for SGD.\n",
    "    epochs : int, default=3\n",
    "        Number of passes over the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.0001, batch_size=20, epochs=3):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.beta = None\n",
    "        self.K = None\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        \"\"\"\n",
    "        Flatten input images to 2D array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Flattened data of shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    def _batchify(self, X, y):\n",
    "        \"\"\"\n",
    "        Split data into batches.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Batched X and y.\n",
    "        \"\"\"\n",
    "        n_batches = X.shape[0] // self.batch_size\n",
    "        X = X[:n_batches*self.batch_size].reshape(n_batches, self.batch_size, *X.shape[1:])\n",
    "        y = y[:n_batches*self.batch_size].reshape(n_batches, self.batch_size, 1)\n",
    "        return X, y\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Numerically stable sigmoid function.\n",
    "        \"\"\"\n",
    "        out = np.where(\n",
    "            x >= 0,\n",
    "            1 / (1 + np.exp(-x)),\n",
    "            np.exp(x) / (1 + np.exp(x))\n",
    "        )\n",
    "        return out.reshape(-1, 1)\n",
    "\n",
    "    def _compute_logits(self, X_flat):\n",
    "        \"\"\"\n",
    "        Compute logits for input data.\n",
    "        \"\"\"\n",
    "        X_flat = np.hstack((np.ones((X_flat.shape[0], 1)), X_flat))\n",
    "        return X_flat @ self.beta, X_flat\n",
    "\n",
    "    def _compute_gradient_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute gradient of the loss function.\n",
    "        \"\"\"\n",
    "        X_flat = self._flatten(X)\n",
    "        logits, X_flat_with_bias = self._compute_logits(X_flat)\n",
    "        p = self._sigmoid(logits)\n",
    "        grad = (p - y).flatten() @ X_flat_with_bias / X_flat.shape[0]\n",
    "        return grad\n",
    "\n",
    "    def fit(self, X, y, initialize_beta=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit the logistic regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Training data.\n",
    "        y : np.ndarray\n",
    "            Training labels.\n",
    "        initialize_beta : bool, default=True\n",
    "            Whether to re-initialize model parameters.\n",
    "        verbose : bool, default=False\n",
    "            If True, print training accuracy after each epoch.\n",
    "        \"\"\"\n",
    "        self.K = np.unique(y).shape[0]\n",
    "        X_batched, y_batched = self._batchify(X, y)\n",
    "        n_features = np.prod(X_batched.shape[2:]).item()\n",
    "        if initialize_beta or self.beta is None:\n",
    "            self.beta = np.random.randn(n_features + 1) * 0.01  # Small random initialization\n",
    "        if verbose == True:\n",
    "            print(f\"Epoch {0}/{self.epochs}, Training accuracy: {self.score(X, y):.2%}\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for j, X_batch in enumerate(X_batched):\n",
    "                y_batch = y_batched[j]\n",
    "                gradient = self._compute_gradient_loss(X_batch, y_batch)\n",
    "                self.beta -= self.learning_rate * gradient\n",
    "\n",
    "            if verbose == True:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Training accuracy: {self.score(X, y):.2%}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        logits, _ = self._compute_logits(self._flatten(X))\n",
    "        p = self._sigmoid(logits)\n",
    "        y_pred = (p > 0.5).astype(int)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** run the cell below. It trains a logistic regression model and computes its accuracy on the training set, if you correctly did the previous exercise. If `verbose=True` it also reports the training accuracy while training.\n",
    "\n",
    "Play with the values of `learning_rate`, `batch_size` and `epochs`. By doing so, try to get a training accuracy of near 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3, Training accuracy: 66.39%\n",
      "Epoch 1/3, Training accuracy: 96.94%\n",
      "Epoch 2/3, Training accuracy: 97.78%\n",
      "Epoch 3/3, Training accuracy: 98.61%\n"
     ]
    }
   ],
   "source": [
    "model = BinaryLogisticRegressionClassifier(learning_rate=0.0001, batch_size=20, epochs=3)\n",
    "model.fit(X, y, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** run the cell below. It uses cross-validation to estimate model performance of the `BinaryLogisticRegressionClassifier` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9983333333333334, 0.0006447649527488834)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BinaryLogisticRegressionClassifier(learning_rate=0.01, batch_size=20, epochs=3)\n",
    "model.cross_validation(X, y, K_cv=5, runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** run the cell below. It computes the confusion matrix using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual class:</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted class:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual class:       0    1\n",
       "Predicted class:          \n",
       "0                 1.0  0.0\n",
       "1                 0.0  1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.confusion_matrix(X, y, K_cv=5, runs=10, class_labels=digits.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: logistic regression (non-binary labels)\n",
    "We now re-load the dataset and include all possible 10 handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations: 1797\n",
      "Number of observations of class 0: 178\n",
      "Number of observations of class 1: 182\n",
      "Number of observations of class 2: 177\n",
      "Number of observations of class 3: 183\n",
      "Number of observations of class 4: 181\n",
      "Number of observations of class 5: 182\n",
      "Number of observations of class 6: 181\n",
      "Number of observations of class 7: 179\n",
      "Number of observations of class 8: 174\n",
      "Number of observations of class 9: 180\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL!!! #\n",
    "digits = datasets.load_digits()\n",
    "X = digits.images\n",
    "y = digits.target.reshape(-1, 1)\n",
    "N = len(y)\n",
    "K = np.unique(y).shape[0]\n",
    "\n",
    "print(f\"Total number of observations: {N}\")\n",
    "for k in range(K):\n",
    "    print(f\"Number of observations of class {k}: {np.sum(y == k)}\")\n",
    "    \n",
    "plot_digits_sample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** create a new class `LogisticRegressionClassifier`, similar to the class `BinaryLogisticRegressionClassifier` defined above, that performs logistic regression on the MNIST dataset loaded in the previous cell. Since this is not a binary classification task, you have to work with a one-hot encoded target variable and a softmax function (instead of a sigmoid function).\n",
    "\n",
    "Use comments inside the code to clearly indicate where and how this class differs from the class `BinaryLogisticRegressionClassifier` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(ClassifierBase):\n",
    "    \"\"\"\n",
    "    Multi-class logistic regression classifier using stochastic gradient descent.\n",
    "\n",
    "    This class implements a multi-class logistic regression model for classification tasks,\n",
    "    using negative log likelihood as the loss function and SGD for optimization.\n",
    "    \n",
    "    Key differences from BinaryLogisticRegressionClassifier:\n",
    "    - Uses one-hot encoding for multi-class targets instead of binary labels\n",
    "    - Implements softmax function instead of sigmoid for multi-class probabilities\n",
    "    - Beta parameter matrix has shape (n_features + 1, n_classes) instead of vector\n",
    "    - Gradient computation handles multiple classes simultaneously\n",
    "\n",
    "    Parameters (stay the same) \n",
    "    ----------\n",
    "    learning_rate : float, default=0.0001\n",
    "        Learning rate for gradient descent.\n",
    "    batch_size : int, default=20\n",
    "        Number of samples per batch for SGD.\n",
    "    epochs : int, default=3\n",
    "        Number of passes over the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.0001, batch_size=20, epochs=3):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.beta = None\n",
    "        self.K = None\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        \"\"\"\n",
    "        Flatten input images to 2D array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Flattened data of shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    def _batchify(self, X, y):\n",
    "        \"\"\"\n",
    "        Split data into batches.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Batched X and y.\n",
    "        \"\"\"\n",
    "        n_batches = X.shape[0] // self.batch_size\n",
    "        X = X[:n_batches*self.batch_size].reshape(n_batches, self.batch_size, *X.shape[1:])\n",
    "        y = y[:n_batches*self.batch_size].reshape(n_batches, self.batch_size, -1)\n",
    "        return X, y\n",
    "    \n",
    "    def _one_hot_encode(self, y):\n",
    "        \"\"\"\n",
    "        Convert labels to one-hot encoded format.\n",
    "\n",
    "        This differs from the binary case by using K classes\n",
    "        instead of binary classification.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.ndarray\n",
    "            Label array of shape (n_samples, 1)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            One-hot encoded labels of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        n_samples = y.shape[0]\n",
    "        y_one_hot = np.zeros((n_samples, self.K))\n",
    "        y_one_hot[np.arange(n_samples), y.flatten()] = 1\n",
    "        return y_one_hot\n",
    "    \n",
    "    def _softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Numerically stable softmax function for multi-class probabilities.\n",
    "        \n",
    "        This replaces sigmoid function to handle multiple classes\n",
    "        instead of binary classification.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : np.ndarray\n",
    "            Logits of shape (n_samples, n_classes)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Softmax probabilities of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    def _compute_logits(self, X_flat):\n",
    "        \"\"\"\n",
    "        Compute logits for input data.\n",
    "        \n",
    "        This returns logits matrix of shape (n_samples, n_classes)\n",
    "        instead of vector for binary classification.\n",
    "        \"\"\"\n",
    "        X_flat = np.hstack((np.ones((X_flat.shape[0], 1)), X_flat))\n",
    "        return X_flat @ self.beta, X_flat\n",
    "\n",
    "    def _compute_gradient_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute gradient of the loss function for multi-class classification.\n",
    "\n",
    "        This computes gradient for multi-class setting using one-hot\n",
    "        encoded targets and softmax probabilities instead of binary sigmoid.\n",
    "        \"\"\"\n",
    "        X_flat = self._flatten(X)\n",
    "        logits, X_flat_with_bias = self._compute_logits(X_flat)\n",
    "        p = self._softmax(logits)  # Shape: (batch_size, K)\n",
    "        y_one_hot = self._one_hot_encode(y)  # Shape: (batch_size, K)\n",
    "        \n",
    "        # Gradient computation for multi-class: X^T @ (p - y_one_hot) / batch_size\n",
    "        grad = X_flat_with_bias.T @ (p - y_one_hot) / X_flat.shape[0]  # Shape: (n_features + 1, K)\n",
    "        return grad\n",
    "\n",
    "    def fit(self, X, y, initialize_beta=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit the logistic regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Training data.\n",
    "        y : np.ndarray\n",
    "            Training labels.\n",
    "        initialize_beta : bool, default=True\n",
    "            Whether to re-initialize model parameters.\n",
    "        verbose : bool, default=False\n",
    "            If True, print training accuracy after each epoch.\n",
    "        \"\"\"\n",
    "        self.K = np.unique(y).shape[0]\n",
    "        X_batched, y_batched = self._batchify(X, y)\n",
    "        n_features = np.prod(X_batched.shape[2:]).item()\n",
    "        \n",
    "        # Difference: Beta is now a matrix of shape (n_features + 1, K) instead of vector\n",
    "        if initialize_beta or self.beta is None:\n",
    "            self.beta = np.random.randn(n_features + 1, self.K) * 0.01\n",
    "            \n",
    "        if verbose == True:\n",
    "            print(f\"Epoch {0}/{self.epochs}, Training accuracy: {self.score(X, y):.2%}\")\n",
    "            \n",
    "        for epoch in range(self.epochs):\n",
    "            for j, X_batch in enumerate(X_batched):\n",
    "                y_batch = y_batched[j]\n",
    "                gradient = self._compute_gradient_loss(X_batch, y_batch)\n",
    "                self.beta -= self.learning_rate * gradient\n",
    "\n",
    "            if verbose == True:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Training accuracy: {self.score(X, y):.2%}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        This uses argmax of softmax probabilities to get predicted class\n",
    "        instead of thresholding sigmoid at 0.5 for binary classification.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Predicted class labels of shape (n_samples, 1).\n",
    "        \"\"\"\n",
    "        logits, _ = self._compute_logits(self._flatten(X))\n",
    "        p = self._softmax(logits)  # Shape: (n_samples, K)\n",
    "        y_pred = np.argmax(p, axis=1).reshape(-1, 1)  # Shape: (n_samples, 1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** use the cell below to show that the class `LogisticRegressionClassifier` works as desired. In particular, show that\n",
    "* it learns to predict the training data during training.\n",
    "* it also learns to predict out-of-sample data (using cross-validation).\n",
    "* it also learns to predict out-of-sample data (using a confusion matrix).\n",
    "\n",
    "Write in a markdown cell explicitly which of the 10 digits is the hardest to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, Training accuracy: 7.90%\n",
      "Epoch 1/10, Training accuracy: 92.82%\n",
      "Epoch 2/10, Training accuracy: 93.60%\n",
      "Epoch 3/10, Training accuracy: 93.99%\n",
      "Epoch 4/10, Training accuracy: 93.99%\n",
      "Epoch 5/10, Training accuracy: 94.05%\n",
      "Epoch 6/10, Training accuracy: 94.32%\n",
      "Epoch 7/10, Training accuracy: 94.71%\n",
      "Epoch 8/10, Training accuracy: 95.27%\n",
      "Epoch 9/10, Training accuracy: 95.38%\n",
      "Epoch 10/10, Training accuracy: 95.55%\n",
      "\n",
      "Final training accuracy: 95.55%\n",
      "\n",
      "Model Performance with cross validation: 95.10%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual class:</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted class:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual class:       0     1     2     3     4     5     6     7     8     9\n",
       "Predicted class:                                                           \n",
       "0                 1.0  0.00  0.00  0.00  0.01  0.00  0.00  0.00  0.01  0.00\n",
       "1                 0.0  0.98  0.02  0.00  0.05  0.01  0.02  0.00  0.15  0.01\n",
       "2                 0.0  0.00  0.95  0.00  0.00  0.01  0.00  0.00  0.01  0.00\n",
       "3                 0.0  0.00  0.03  0.97  0.00  0.01  0.00  0.01  0.08  0.01\n",
       "4                 0.0  0.00  0.00  0.00  0.90  0.00  0.00  0.00  0.00  0.00\n",
       "5                 0.0  0.00  0.00  0.00  0.00  0.91  0.00  0.00  0.01  0.01\n",
       "6                 0.0  0.00  0.00  0.00  0.03  0.01  0.97  0.00  0.01  0.00\n",
       "7                 0.0  0.00  0.00  0.00  0.01  0.00  0.00  0.97  0.01  0.00\n",
       "8                 0.0  0.00  0.00  0.01  0.00  0.00  0.00  0.00  0.67  0.00\n",
       "9                 0.0  0.01  0.00  0.02  0.02  0.05  0.00  0.02  0.05  0.97"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegressionClassifier with training progress\n",
    "np.random.seed(9)\n",
    "model = LogisticRegressionClassifier(learning_rate=0.01, batch_size=20, epochs=10)\n",
    "model.fit(X, y, verbose=True)\n",
    "\n",
    "print(f\"\\nFinal training accuracy: {model.score(X, y):.2%}\")\n",
    "\n",
    "# Model performance with out of sample data \n",
    "np.random.seed(9)\n",
    "model = LogisticRegressionClassifier(learning_rate=0.01, batch_size=20, epochs=10)\n",
    "model.cross_validation(X, y, K_cv=5, runs=10)\n",
    "\n",
    "print(f\"\\nModel Performance with cross validation: {model.score(X, y):.2%}\")\n",
    "\n",
    "# Model performance with confusion matrix\n",
    "np.random.seed(9)\n",
    "model.confusion_matrix(X, y, K_cv=5, runs=10, class_labels=digits.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix it can be seen that the number 8 is the most difficult to predict, in fact it has the lowest accuracy with only 67% being correctly classified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** the dataset we have worked with so far is a simplification of the original MNIST dataset, as the resolution is 8x8 instead of 28x28. Use the cell below to download and import the original MNIST dataset. This can be done, for example, with the `sklear` package. The data must be saved in a folder called `data`, which must be created if it does not exist yet. \n",
    "\n",
    "Note that this dataset has a default training set (the first 60000 observations) and test set (the last 10000 observations). At the end of the cell, organize the data in objects `X_train`, `y_train`, `X_test`, `y_test`, such that these objects have the same dimensional structure as used earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: X_train shape (60000, 28, 28), y_train shape (60000, 1)\n",
      "Test set: X_test shape (10000, 28, 28), y_test shape (10000, 1)\n",
      "Number of unique classes: 10\n",
      "\n",
      "Training set class distribution:\n",
      "Total training observations: 60000\n",
      "Total test observations: 10000\n",
      "Class 0: 5923 samples\n",
      "Class 1: 6742 samples\n",
      "Class 2: 5958 samples\n",
      "Class 3: 6131 samples\n",
      "Class 4: 5842 samples\n",
      "Class 5: 5421 samples\n",
      "Class 6: 5918 samples\n",
      "Class 7: 6265 samples\n",
      "Class 8: 5851 samples\n",
      "Class 9: 5949 samples\n"
     ]
    }
   ],
   "source": [
    "# original MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True, data_home='data')\n",
    "\n",
    "# Extract features and targets\n",
    "X_full = mnist.data.values\n",
    "y_full = mnist.target.values.astype(int)\n",
    "\n",
    "# Split into training and test sets (first 60000 for training, last 10000 for test)\n",
    "X_train = X_full[:60000].reshape(60000, 28, 28)  # Shape: (60000, 28, 28)\n",
    "y_train = y_full[:60000].reshape(-1, 1)          # Shape: (60000, 1)\n",
    "X_test = X_full[60000:].reshape(10000, 28, 28)   # Shape: (10000, 28, 28)\n",
    "y_test = y_full[60000:].reshape(-1, 1)           # Shape: (10000, 1)\n",
    "\n",
    "print(f\"Training set: X_train shape {X_train.shape}, y_train shape {y_train.shape}\")\n",
    "print(f\"Test set: X_test shape {X_test.shape}, y_test shape {y_test.shape}\")\n",
    "print(f\"Number of unique classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Display class distribution in training set\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(f\"Total training observations: {len(y_train)}\")\n",
    "print(f\"Total test observations: {len(y_test)}\")\n",
    "for k in range(10):\n",
    "    print(f\"Class {k}: {np.sum(y_train == k)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** show that the training accuracy of a logistic regression classifier on the MNIST dataset typically converges to about 88%. Furthermore, show that this is also the case for the test accuracy. (Hint: `ClassifierBase` has a built-in method called `score`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30, Training accuracy: 5.09%\n",
      "Epoch 1/30, Training accuracy: 83.43%\n",
      "Epoch 2/30, Training accuracy: 86.93%\n",
      "Epoch 3/30, Training accuracy: 87.24%\n",
      "Epoch 4/30, Training accuracy: 85.23%\n",
      "Epoch 5/30, Training accuracy: 88.29%\n",
      "Epoch 6/30, Training accuracy: 87.02%\n",
      "Epoch 7/30, Training accuracy: 88.52%\n",
      "Epoch 8/30, Training accuracy: 89.21%\n",
      "Epoch 9/30, Training accuracy: 89.00%\n",
      "Epoch 10/30, Training accuracy: 88.74%\n",
      "Epoch 11/30, Training accuracy: 86.12%\n",
      "Epoch 12/30, Training accuracy: 89.36%\n",
      "Epoch 13/30, Training accuracy: 87.78%\n",
      "Epoch 14/30, Training accuracy: 85.96%\n",
      "Epoch 15/30, Training accuracy: 87.28%\n",
      "Epoch 16/30, Training accuracy: 87.49%\n",
      "Epoch 17/30, Training accuracy: 88.28%\n",
      "Epoch 18/30, Training accuracy: 89.82%\n",
      "Epoch 19/30, Training accuracy: 86.08%\n",
      "Epoch 20/30, Training accuracy: 90.03%\n",
      "Epoch 21/30, Training accuracy: 88.67%\n",
      "Epoch 22/30, Training accuracy: 88.10%\n",
      "Epoch 23/30, Training accuracy: 90.32%\n",
      "Epoch 24/30, Training accuracy: 87.27%\n",
      "Epoch 25/30, Training accuracy: 86.15%\n",
      "Epoch 26/30, Training accuracy: 89.63%\n",
      "Epoch 27/30, Training accuracy: 89.00%\n",
      "Epoch 28/30, Training accuracy: 88.67%\n",
      "Epoch 29/30, Training accuracy: 89.89%\n",
      "Epoch 30/30, Training accuracy: 89.28%\n",
      "\n",
      "Final training accuracy: 89.28%\n",
      "Final test accuracy: 88.29%\n"
     ]
    }
   ],
   "source": [
    "# Train model on MNIST training set and evaluate on both training and test sets\n",
    "np.random.seed(9)\n",
    "model = LogisticRegressionClassifier(learning_rate=0.01, batch_size=100, epochs=30)\n",
    "model.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "# Calculate training and test accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nFinal training accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"Final test accuracy: {test_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** use a single line of code to determine which of the 10 digest are the hardest to predict. Use the test set for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardest digit to predict: 8\n"
     ]
    }
   ],
   "source": [
    "# Hardest digit to predict is the lowest number on the diagonal in the confusion matrix \n",
    "hardest_digit = model.confusion_matrix(X_test, y_test, K_cv=5, runs=10, class_labels=digits.target_names).values.diagonal().argmin()\n",
    "print(f\"Hardest digit to predict: {hardest_digit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: modeling competition\n",
    "This part of the assignment consists of a modeling competition. In the cell below a dataset `data_train` is loaded, consisting of 1000 independent observations with 2000-dimensional continuous input features `data_train.X` and a binary target variable `data_train.y`. It also contains another binary variable `data_train.a`.\n",
    "\n",
    "Your goal is to use this data to train a model with the best generalization performance. This model uses the input features $X$ to predict the target variable $y.$ The additional label $a$ may not be used to predict $y.$ The names of the students with the 3 best performing models will be announced.\n",
    "\n",
    "This part of the assignment consists of three subsections:\n",
    "* *analysis*: in this subsection you should analyze the data and try different modeling techniques and compare them in the correct way. You can use as many cells (code, markdown) as you need. Make sure your analysis is structured logically. It should be readable like a report or essay.\n",
    "* *best model*: based on your analysis in the previous subsection, you have identified a best model. This should be defined in the code cell of this subsection, in the form of a class called `Model`.\n",
    "* *generalization performance*: in this subsection the generalization performance of your best model `Model` is assessed. This is done with a test set consisting of 1000 independent datapoints. Students do not have access to this test set. It is **important** to note that the test set is **not** drawn from the same distribution as the train set. One can partition the data in four subgroups: ($y=0, a=0$), ($y=0, a=1$), ($y=1, a=0$), ($y=1, a=1$). In the training set the occurrence frequency of the subgroups is unequal, while for the test set it is equal. On the other hand, the marginal distribution of $(X,y)$ is the same for both the training and test set. We call the relation between $y$ and $a$ a \"spurious correlation\". This means that predicting $y$ using $X$ for the test set can be learned with the training set, but it is important that the generalization performance is good for all subgroups.\n",
    "\n",
    "Details of the modeling competition:\n",
    "* you are only allowed to use methods that have been discussed in this course.\n",
    "* the cell in the subsection *generalization performance* should **NOT** be changed!!! Not that if the test set file `data_test.pkl` is not available (which is the case for students), a file called `data_test_FAKE.pkl` is loaded instead. The data in this file has exactly the same structure as the data in `data_test.pkl`, but the actual numbers are completely random. You can use this file to test whether the cell in the subsection *generalization performance* runs without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL!!! #\n",
    "class DataSimple():\n",
    "    \"\"\"Simplified Data class without DGP reference, noise and masks.\"\"\"\n",
    "    \n",
    "    def __init__(self, y: np.ndarray, a: np.ndarray, x: np.ndarray, random_state: int = None):\n",
    "        \"\"\"\n",
    "        Initialize the DataSimple object.\n",
    "\n",
    "        Args:\n",
    "            y: Binary outcome variable\n",
    "            a: Attribute variable\n",
    "            x: Composite variable\n",
    "            random_state: Seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.n = y.shape[0]\n",
    "        self.y = y\n",
    "        self.a = a\n",
    "        self.X = x\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_shapes(self) -> dict:\n",
    "        \"\"\"Return the shapes of y, a, and x.\"\"\"\n",
    "        return {\n",
    "            'y_shape': self.y.shape,\n",
    "            'a_shape': self.a.shape,\n",
    "            'x_shape': self.X.shape\n",
    "        }\n",
    "\n",
    "with open('data/data_train.pkl', 'rb') as f:\n",
    "    data_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Use the space below for your analysis of this modeling problem. You can use as many cells (code, markdown) as you need. Make sure your analysis is structured logically. It should be readable like a report or essay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the best model the following procedures will be implemented: \n",
    "\n",
    "1. Data Analysis and visualisation \n",
    "2. Model Evaluations \n",
    "    - Lasso Regression \n",
    "    - Ridge Regression \n",
    "    - PCA \n",
    "    - Logistic Regression \n",
    "    - Bayesian \n",
    "3. Some of the models will be combined using model averaging in order to improve robustness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "X shape: (1000, 2000)\n",
      "y shape: (1000, 1)\n",
      "a shape: (1000, 1)\n",
      "\n",
      "Correlation between y and a: 0.8065\n",
      "\n",
      "Class distribution for y:\n",
      "y=0: 506 samples (50.6%)\n",
      "y=1: 494 samples (49.4%)\n",
      "\n",
      "Class distribution for a:\n",
      "a=0: 525 samples (52.5%)\n",
      "a=1: 475 samples (47.5%)\n",
      "\n",
      "Cross-tabulation of y and a:\n",
      "y=0, a=0: 467 samples\n",
      "y=0, a=1: 39 samples\n",
      "y=1, a=0: 58 samples\n",
      "y=1, a=1: 436 samples\n",
      "\n",
      "Feature statistics:\n",
      "Mean of means: 0.0002\n",
      "Mean of variances: 0.2643\n",
      "Min feature mean: -0.0596\n",
      "Max feature mean: 0.0562\n",
      "Min feature variance: 0.2200\n",
      "Max feature variance: 0.4077\n"
     ]
    }
   ],
   "source": [
    "# Check shapes of training data\n",
    "print(\"Data shapes:\")\n",
    "print(f\"X shape: {data_train.X.shape}\")\n",
    "print(f\"y shape: {data_train.y.shape}\")\n",
    "print(f\"a shape: {data_train.a.shape}\")\n",
    "\n",
    "# Check correlation between y and a\n",
    "correlation_ya = np.corrcoef(data_train.y.flatten(), data_train.a.flatten())[0, 1]\n",
    "print(f\"\\nCorrelation between y and a: {correlation_ya:.4f}\")\n",
    "\n",
    "# Check class distributions\n",
    "print(f\"\\nClass distribution for y:\")\n",
    "unique_y, counts_y = np.unique(data_train.y, return_counts=True)\n",
    "for val, count in zip(unique_y, counts_y):\n",
    "    print(f\"y={val}: {count} samples ({count/len(data_train.y)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass distribution for a:\")\n",
    "unique_a, counts_a = np.unique(data_train.a, return_counts=True)\n",
    "for val, count in zip(unique_a, counts_a):\n",
    "    print(f\"a={val}: {count} samples ({count/len(data_train.a)*100:.1f}%)\")\n",
    "\n",
    "# Cross-tabulation of y and a\n",
    "print(f\"\\nCross-tabulation of y and a:\")\n",
    "for y_val in unique_y:\n",
    "    for a_val in unique_a:\n",
    "        count = np.sum((data_train.y == y_val) & (data_train.a == a_val))\n",
    "        print(f\"y={y_val}, a={a_val}: {count} samples\")\n",
    "\n",
    "# Calculate mean and variance of X features\n",
    "X_mean = np.mean(data_train.X, axis=0)\n",
    "X_var = np.var(data_train.X, axis=0)\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"Mean of means: {np.mean(X_mean):.4f}\")\n",
    "print(f\"Mean of variances: {np.mean(X_var):.4f}\")\n",
    "print(f\"Min feature mean: {np.min(X_mean):.4f}\")\n",
    "print(f\"Max feature mean: {np.max(X_mean):.4f}\")\n",
    "print(f\"Min feature variance: {np.min(X_var):.4f}\")\n",
    "print(f\"Max feature variance: {np.max(X_var):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can deduce that there is a strong correlation between y and a, in fact in the four subgroups, we can see that most of the samples have a = y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X statistics:\n",
      "Mean: 0.0002\n",
      "Std: 0.5144\n",
      "\n",
      "Standardized X statistics:\n",
      "Mean: 0.0000\n",
      "Std: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features (mean=0, variance=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(data_train.X)\n",
    "\n",
    "# checking if the standarization worked\n",
    "print(f\"Original X statistics:\")\n",
    "print(f\"Mean: {np.mean(data_train.X):.4f}\")\n",
    "print(f\"Std: {np.std(data_train.X):.4f}\")\n",
    "\n",
    "print(f\"\\nStandardized X statistics:\")\n",
    "print(f\"Mean: {np.mean(X_standardized):.4f}\")\n",
    "print(f\"Std: {np.std(X_standardized):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a correlation between a and y, and the test set has equal subgroup frequencies (unlike the training set as we've seen above), the following metrics will be compared for model evaluation: \n",
    "\n",
    "- worst group accuracy i.e. performance of the poorest performing subgroup \n",
    "- balanced group accuracy across all subgroups \n",
    "\n",
    "These will be determined using a class that uses cross validation on a model and presents these outputs for comparison reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def evaluate_model_with_subgroups(model_class, data, cv_folds=5, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate a model using cross-validation with subgroup-aware metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_class : class\n",
    "        The model class to evaluate\n",
    "    data : DataSimple\n",
    "        The data object containing X, y, and a\n",
    "    cv_folds : int\n",
    "        Number of CV folds\n",
    "    **model_kwargs : dict\n",
    "        Parameters to pass to model constructor\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results containing metrics\n",
    "    \"\"\"\n",
    "    X = data.X\n",
    "    y = data.y\n",
    "    a = data.a\n",
    "    \n",
    "    # Create stratified folds ensuring all subgroups are represented\n",
    "    composite_y = y.flatten() * 2 + a.flatten()  # Creates 4 unique labels for subgroups\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = {\n",
    "        'worst_group_acc': [],\n",
    "        'balanced_group_acc': [],\n",
    "        'overall_acc': []\n",
    "    }\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, composite_y):\n",
    "        # Split data\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        a_train, a_val = a[train_idx], a[val_idx]\n",
    "        \n",
    "        # Create DataSimple objects for training and validation\n",
    "        train_data = DataSimple(y_train, a_train, X_train)\n",
    "        \n",
    "        # Initialize and train model\n",
    "        model = model_class(**model_kwargs)\n",
    "        model.fit(train_data)\n",
    "        \n",
    "        # Get predictions on validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate subgroup accuracies\n",
    "        subgroup_accs = []\n",
    "        for y_class in [0, 1]:\n",
    "            for a_class in [0, 1]:\n",
    "                mask = (y_val.flatten() == y_class) & (a_val.flatten() == a_class)\n",
    "                if np.sum(mask) > 0:\n",
    "                    acc = np.mean(y_pred[mask] == y_val[mask])\n",
    "                    subgroup_accs.append(acc)\n",
    "        \n",
    "        # Store metrics for this fold\n",
    "        if subgroup_accs:\n",
    "            fold_results['worst_group_acc'].append(min(subgroup_accs))\n",
    "            fold_results['balanced_group_acc'].append(np.mean(subgroup_accs))\n",
    "            fold_results['overall_acc'].append(np.mean(y_pred.flatten() == y_val.flatten()))\n",
    "    \n",
    "    return {\n",
    "        'worst_group_mean': np.mean(fold_results['worst_group_acc']),\n",
    "        'worst_group_std': np.std(fold_results['worst_group_acc']),\n",
    "        'balanced_group_mean': np.mean(fold_results['balanced_group_acc']),\n",
    "        'balanced_group_std': np.std(fold_results['balanced_group_acc']),\n",
    "        'overall_mean': np.mean(fold_results['overall_acc']),\n",
    "        'overall_std': np.std(fold_results['overall_acc'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Ridge Regression** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeLogisticRegression():\n",
    "    \"\"\"\n",
    "    Ridge Logistic Regression for binary classification with L2 regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = None\n",
    "        self.feature_mean = None\n",
    "        self.feature_std = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "        # Add clipping to prevent extreme values\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return np.where(z >= 0, \n",
    "                       1 / (1 + np.exp(-z)), \n",
    "                       np.exp(z) / (1 + np.exp(z)))\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the ridge logistic regression model.\"\"\"\n",
    "        X = data.X\n",
    "        y = data.y.flatten()\n",
    "        \n",
    "        # Standardize features\n",
    "        self.feature_mean = np.mean(X, axis=0)\n",
    "        self.feature_std = np.std(X, axis=0) + 1e-8\n",
    "        X_std = (X - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # Add intercept term\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        n_features = X_aug.shape[1]\n",
    "        \n",
    "        # Initialize parameters (small random values often work better)\n",
    "        self.beta = np.random.normal(0, 0.01, n_features)\n",
    "        \n",
    "        # Gradient descent with L2 regularization\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Predictions\n",
    "            z = X_aug @ self.beta\n",
    "            p = self._sigmoid(z)\n",
    "            \n",
    "            # Gradient with L2 penalty (don't regularize intercept)\n",
    "            gradient = X_aug.T @ (p - y) / len(y)\n",
    "            gradient[1:] += self.alpha * self.beta[1:]  # L2 penalty for weights (not intercept)\n",
    "            \n",
    "            # Update parameters\n",
    "            beta_old = self.beta.copy()\n",
    "            self.beta -= self.learning_rate * gradient\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(self.beta - beta_old) < self.tol:\n",
    "                break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict binary labels.\"\"\"\n",
    "        X_std = (X - self.feature_mean) / self.feature_std\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.beta\n",
    "        probabilities = self._sigmoid(z)\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "        return predictions.reshape(-1, 1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        X_std = (X - self.feature_mean) / self.feature_std\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.beta\n",
    "        return self._sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoLogisticRegression():\n",
    "    \"\"\"\n",
    "    Lasso Logistic Regression for binary classification with L1 regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = None\n",
    "        self.feature_mean = None\n",
    "        self.feature_std = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "        # Add clipping to prevent extreme values\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return np.where(z >= 0, \n",
    "                       1 / (1 + np.exp(-z)), \n",
    "                       np.exp(z) / (1 + np.exp(z)))\n",
    "    \n",
    "    def _soft_thresholding(self, x, threshold):\n",
    "        \"\"\"Soft thresholding operator for L1 regularization.\"\"\"\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the lasso logistic regression model.\"\"\"\n",
    "        X = data.X\n",
    "        y = data.y.flatten()\n",
    "        \n",
    "        # Standardize features\n",
    "        self.feature_mean = np.mean(X, axis=0)\n",
    "        self.feature_std = np.std(X, axis=0) + 1e-8\n",
    "        X_std = (X - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # Add intercept term\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        n_features = X_aug.shape[1]\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.beta = np.random.normal(0, 0.01, n_features)\n",
    "        \n",
    "        # Gradient descent with L1 regularization using proximal gradient method\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Predictions\n",
    "            z = X_aug @ self.beta\n",
    "            p = self._sigmoid(z)\n",
    "            \n",
    "            # Gradient (without L1 penalty)\n",
    "            gradient = X_aug.T @ (p - y) / len(y)\n",
    "            \n",
    "            # Update parameters\n",
    "            beta_old = self.beta.copy()\n",
    "            \n",
    "            # Standard gradient step\n",
    "            beta_temp = self.beta - self.learning_rate * gradient\n",
    "            \n",
    "            # Apply soft thresholding for L1 regularization (don't regularize intercept)\n",
    "            self.beta[0] = beta_temp[0]  # No regularization for intercept\n",
    "            self.beta[1:] = self._soft_thresholding(beta_temp[1:], self.learning_rate * self.alpha)\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(self.beta - beta_old) < self.tol:\n",
    "                break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict binary labels.\"\"\"\n",
    "        X_std = (X - self.feature_mean) / self.feature_std\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.beta\n",
    "        probabilities = self._sigmoid(z)\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "        return predictions.reshape(-1, 1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        X_std = (X - self.feature_mean) / self.feature_std\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.beta\n",
    "        return self._sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. PCA Logistic Regression** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCALogisticRegression():\n",
    "    \"\"\"\n",
    "    PCA + Logistic Regression for binary classification with dimensionality reduction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=50, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = None\n",
    "        self.feature_mean = None\n",
    "        self.components = None\n",
    "        self.explained_variance = None\n",
    "        self.pca_mean = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return np.where(z >= 0, \n",
    "                       1 / (1 + np.exp(-z)), \n",
    "                       np.exp(z) / (1 + np.exp(z)))\n",
    "    \n",
    "    def _fit_pca(self, X):\n",
    "        \"\"\"Fit PCA to the data.\"\"\"\n",
    "        # Center the data\n",
    "        self.pca_mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.pca_mean\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = np.cov(X_centered.T)\n",
    "        \n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        # Sort eigenvalues and eigenvectors in descending order\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Select top n_components\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "        self.explained_variance = eigenvalues[:self.n_components]\n",
    "    \n",
    "    def _transform_pca(self, X):\n",
    "        \"\"\"Transform data using fitted PCA.\"\"\"\n",
    "        X_centered = X - self.pca_mean\n",
    "        return X_centered @ self.components\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the PCA + logistic regression model.\"\"\"\n",
    "        X = data.X\n",
    "        y = data.y.flatten()\n",
    "        \n",
    "        # Fit PCA and transform data\n",
    "        self._fit_pca(X)\n",
    "        X_pca = self._transform_pca(X)\n",
    "        \n",
    "        # Standardize PCA-transformed features\n",
    "        self.feature_mean = np.mean(X_pca, axis=0)\n",
    "        self.feature_std = np.std(X_pca, axis=0) + 1e-8\n",
    "        X_std = (X_pca - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # Add intercept term\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        n_features = X_aug.shape[1]\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.beta = np.random.normal(0, 0.01, n_features)\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Predictions\n",
    "            z = X_aug @ self.beta\n",
    "            p = self._sigmoid(z)\n",
    "            \n",
    "            # Gradient\n",
    "            gradient = X_aug.T @ (p - y) / len(y)\n",
    "            \n",
    "            # Update parameters\n",
    "            beta_old = self.beta.copy()\n",
    "            self.beta -= self.learning_rate * gradient\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(self.beta - beta_old) < self.tol:\n",
    "                break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict binary labels.\"\"\"\n",
    "        # Transform using PCA\n",
    "        X_pca = self._transform_pca(X)\n",
    "        \n",
    "        # Standardize\n",
    "        X_std = (X_pca - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # Add intercept and predict\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.beta\n",
    "        probabilities = self._sigmoid(z)\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "        return predictions.reshape(-1, 1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        # Transform using PCA\n",
    "        X_pca = self._transform_pca(X)\n",
    "        \n",
    "        # Standardize\n",
    "        X_std = (X_pca - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # Add intercept and predict\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.beta\n",
    "        return self._sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Logistic Regression Classifier** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionForCompetition(ClassifierBase):\n",
    "    \"\"\"\n",
    "    Adapted Logistic Regression Classifier for the competition data format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, batch_size=50, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.beta = None\n",
    "        self.feature_mean = None\n",
    "        self.feature_std = None\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        \"\"\"Flatten input to 2D array if needed.\"\"\"\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return np.where(x >= 0, \n",
    "                       1 / (1 + np.exp(-x)), \n",
    "                       np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "    def fit(self, data, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit the logistic regression model using DataSimple object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : DataSimple\n",
    "            Training data object with X and y attributes\n",
    "        verbose : bool, default=False\n",
    "            If True, print training accuracy after each epoch.\n",
    "        \"\"\"\n",
    "        X = data.X\n",
    "        y = data.y\n",
    "        \n",
    "        # Flatten and standardize features\n",
    "        X_flat = self._flatten(X)\n",
    "        self.feature_mean = np.mean(X_flat, axis=0)\n",
    "        self.feature_std = np.std(X_flat, axis=0) + 1e-8\n",
    "        X_std = (X_flat - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # Add bias term\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        n_features = X_aug.shape[1]\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.beta = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        # Flatten y for consistency\n",
    "        y_flat = y.flatten()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {0}/{self.epochs}, Training accuracy: {self.score(data):.2%}\")\n",
    "            \n",
    "        # Mini-batch gradient descent\n",
    "        n_samples = X_aug.shape[0]\n",
    "        n_batches = max(1, n_samples // self.batch_size)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_aug[indices]\n",
    "            y_shuffled = y_flat[indices]\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * self.batch_size\n",
    "                end_idx = min((i + 1) * self.batch_size, n_samples)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = X_batch @ self.beta\n",
    "                predictions = self._sigmoid(logits)\n",
    "                \n",
    "                # Compute gradient\n",
    "                error = predictions - y_batch\n",
    "                gradient = X_batch.T @ error / X_batch.shape[0]\n",
    "                \n",
    "                # Update parameters\n",
    "                self.beta -= self.learning_rate * gradient\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Training accuracy: {self.score(data):.2%}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
    "        # Flatten and standardize using training statistics\n",
    "        X_flat = self._flatten(X)\n",
    "        X_std = (X_flat - self.feature_mean) / self.feature_std\n",
    "        \n",
    "        # Add bias term\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        \n",
    "        # Make predictions\n",
    "        logits = X_aug @ self.beta\n",
    "        probabilities = self._sigmoid(logits)\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "        \n",
    "        return predictions.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Bayesian Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianClassifier():\n",
    "    \"\"\"\n",
    "    Bayesian Classifier for binary classification using Gaussian assumption.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.class_means = {}\n",
    "        self.class_vars = {}\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the Bayesian classifier.\"\"\"\n",
    "        X = data.X\n",
    "        y = data.y.flatten()\n",
    "        \n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # Calculate priors and parameters for each class\n",
    "        for c in self.classes:\n",
    "            class_mask = (y == c)\n",
    "            \n",
    "            # Prior probability\n",
    "            self.class_priors[c] = np.sum(class_mask) / len(y)\n",
    "            \n",
    "            # Mean and variance for each feature\n",
    "            X_class = X[class_mask]\n",
    "            self.class_means[c] = np.mean(X_class, axis=0)\n",
    "            self.class_vars[c] = np.var(X_class, axis=0) + 1e-8  # Add small value for numerical stability\n",
    "    \n",
    "    def _log_likelihood(self, X, class_label):\n",
    "        \"\"\"Calculate log likelihood for a given class.\"\"\"\n",
    "        mean = self.class_means[class_label]\n",
    "        var = self.class_vars[class_label]\n",
    "        \n",
    "        # Gaussian log likelihood\n",
    "        log_likelihood = -0.5 * np.sum(np.log(2 * np.pi * var), axis=0)\n",
    "        log_likelihood -= 0.5 * np.sum((X - mean)**2 / var, axis=1)\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for x in X:\n",
    "            class_posteriors = {}\n",
    "            \n",
    "            for c in self.classes:\n",
    "                # Log posterior = log prior + log likelihood\n",
    "                log_prior = np.log(self.class_priors[c])\n",
    "                log_likelihood = self._log_likelihood(x.reshape(1, -1), c)\n",
    "                class_posteriors[c] = log_prior + log_likelihood[0]\n",
    "            \n",
    "            # Predict class with highest posterior\n",
    "            predicted_class = max(class_posteriors, key=class_posteriors.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return np.array(predictions).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing any hyperparameter tuning or model averaging, we will compare the preliminary results of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Results:\n",
      "================================================================================\n",
      "\n",
      "Ridge Regression:\n",
      "  Worst Group Accuracy: 0.070  0.085\n",
      "  Balanced Group Accuracy: 0.559  0.047\n",
      "  Overall Accuracy: 0.908  0.015\n",
      "\n",
      "Lasso Regression:\n",
      "  Worst Group Accuracy: 0.017  0.033\n",
      "  Balanced Group Accuracy: 0.501  0.022\n",
      "  Overall Accuracy: 0.854  0.010\n",
      "\n",
      "PCA + Logistic Regression:\n",
      "  Worst Group Accuracy: 0.132  0.125\n",
      "  Balanced Group Accuracy: 0.589  0.059\n",
      "  Overall Accuracy: 0.911  0.022\n",
      "\n",
      "Bayesian Classifier:\n",
      "  Worst Group Accuracy: 0.035  0.043\n",
      "  Balanced Group Accuracy: 0.520  0.029\n",
      "  Overall Accuracy: 0.892  0.015\n",
      "\n",
      "Logistic Regression:\n",
      "  Worst Group Accuracy: 0.191  0.116\n",
      "  Balanced Group Accuracy: 0.635  0.068\n",
      "  Overall Accuracy: 0.919  0.015\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(9)\n",
    "\n",
    "models_to_evaluate = [\n",
    "    ('Ridge Regression', RidgeLogisticRegression, {'alpha': 1.0}),\n",
    "    ('Lasso Regression', LassoLogisticRegression, {'alpha': 0.1}),\n",
    "    ('PCA + Logistic Regression', PCALogisticRegression, {'n_components': 50}),\n",
    "    ('Bayesian Classifier', BayesianClassifier, {}),\n",
    "    ('Logistic Regression', LogisticRegressionForCompetition, {'learning_rate': 0.01})\n",
    "]\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, model_class, params in models_to_evaluate:\n",
    "    print(f\"\\n{name}:\")\n",
    "    try:\n",
    "        results = evaluate_model_with_subgroups(model_class, data_train, cv_folds=5, **params)\n",
    "        print(f\"  Worst Group Accuracy: {results['worst_group_mean']:.3f}  {results['worst_group_std']:.3f}\")\n",
    "        print(f\"  Balanced Group Accuracy: {results['balanced_group_mean']:.3f}  {results['balanced_group_std']:.3f}\")\n",
    "        print(f\"  Overall Accuracy: {results['overall_mean']:.3f}  {results['overall_std']:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that the best worst group accuracy is when using Logistic Regression followed by PCA with logistic regression. Both also have a high overall accuracy so these two methods will be looked at further through hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Logistic Regression hyperparameters\n",
      "============================================================\n",
      "Best Logistic Regression hyperparameters:\n",
      "  Learning Rate: 0.1\n",
      "  Batch Size: 20\n",
      "  Epochs: 10\n",
      "\n",
      "Best Results:\n",
      "  Worst Group Accuracy: 0.304  0.158\n",
      "  Balanced Group Accuracy: 0.687  0.065\n",
      "  Overall Accuracy: 0.928  0.014\n"
     ]
    }
   ],
   "source": [
    "# Tuning logistic regression \n",
    "\n",
    "print(\"Tuning Logistic Regression hyperparameters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "batch_sizes = [20, 50, 100]\n",
    "epochs_list = [10, 20, 30, 50]\n",
    "\n",
    "best_worst_group_acc = 0\n",
    "best_params = {}\n",
    "best_results = {}\n",
    "\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for epochs in epochs_list:\n",
    "            try:\n",
    "                # Evaluate model with current hyperparameters\n",
    "                results = evaluate_model_with_subgroups(\n",
    "                    LogisticRegressionForCompetition, \n",
    "                    data_train, \n",
    "                    cv_folds=5,\n",
    "                    learning_rate=lr,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs\n",
    "                )\n",
    "                \n",
    "                # Check if this is the best worst group accuracy so far\n",
    "                if results['worst_group_mean'] > best_worst_group_acc:\n",
    "                    best_worst_group_acc = results['worst_group_mean']\n",
    "                    best_params = {\n",
    "                        'learning_rate': lr,\n",
    "                        'batch_size': batch_size,\n",
    "                        'epochs': epochs\n",
    "                    }\n",
    "                    best_results = results\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error with lr={lr}, batch_size={batch_size}, epochs={epochs}: {str(e)}\")\n",
    "\n",
    "# Print best results\n",
    "print(f\"Best Logistic Regression hyperparameters:\")\n",
    "print(f\"  Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"  Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"  Epochs: {best_params['epochs']}\")\n",
    "print(f\"\\nBest Results:\")\n",
    "print(f\"  Worst Group Accuracy: {best_results['worst_group_mean']:.3f}  {best_results['worst_group_std']:.3f}\")\n",
    "print(f\"  Balanced Group Accuracy: {best_results['balanced_group_mean']:.3f}  {best_results['balanced_group_std']:.3f}\")\n",
    "print(f\"  Overall Accuracy: {best_results['overall_mean']:.3f}  {best_results['overall_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already a great improvement from before, the overall accuracy increased and the worst group is now 26% accuracy which is already better. \n",
    "\n",
    "Now the PCA + logistic regression will be tuned and compared to this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning PCA + Logistic Regression hyperparameters\n",
      "============================================================\n",
      "Best PCA + Logistic Regression hyperparameters:\n",
      "  N Components: 250\n",
      "  Learning Rate: 0.001\n",
      "  Max Iterations: 1000\n",
      "\n",
      "Best Results:\n",
      "  Worst Group Accuracy: 0.282  0.222\n",
      "  Balanced Group Accuracy: 0.667  0.099\n",
      "  Overall Accuracy: 0.917  0.025\n",
      "\n",
      "Comparison:\n",
      "Best Logistic Regression Worst Group Acc: 0.304\n",
      "Best PCA + Logistic Regression Worst Group Acc: 0.282\n"
     ]
    }
   ],
   "source": [
    "# Tuning PCA + Logistic Regression hyperparameters\n",
    "print(\"Tuning PCA + Logistic Regression hyperparameters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define hyperparameter ranges (improved ranges based on previous results)\n",
    "n_components_range = [30, 50, 100, 150, 200, 250]\n",
    "learning_rates_pca = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "max_iters_range = [1000, 1500, 2000]\n",
    "\n",
    "best_worst_group_acc_pca = 0\n",
    "best_params_pca = {}\n",
    "best_results_pca = {}\n",
    "\n",
    "# Grid search through all combinations\n",
    "for n_comp in n_components_range:\n",
    "    for lr_pca in learning_rates_pca:\n",
    "        for max_iter_val in max_iters_range:\n",
    "            try:\n",
    "                # Evaluate model with current hyperparameters\n",
    "                results = evaluate_model_with_subgroups(\n",
    "                    PCALogisticRegression, \n",
    "                    data_train, \n",
    "                    cv_folds=5,\n",
    "                    n_components=n_comp,\n",
    "                    learning_rate=lr_pca,\n",
    "                    max_iter=max_iter_val\n",
    "                )\n",
    "                \n",
    "                # Check if this is the best worst group accuracy so far\n",
    "                if results['worst_group_mean'] > best_worst_group_acc_pca:\n",
    "                    best_worst_group_acc_pca = results['worst_group_mean']\n",
    "                    best_params_pca = {\n",
    "                        'n_components': n_comp,\n",
    "                        'learning_rate': lr_pca,\n",
    "                        'max_iter': max_iter_val\n",
    "                    }\n",
    "                    best_results_pca = results\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error with n_comp={n_comp}, lr={lr_pca}, max_iter={max_iter_val}: {str(e)}\")\n",
    "   \n",
    "# Print best results for PCA + Logistic Regression\n",
    "print(f\"Best PCA + Logistic Regression hyperparameters:\")\n",
    "print(f\"  N Components: {best_params_pca['n_components']}\")\n",
    "print(f\"  Learning Rate: {best_params_pca['learning_rate']}\")\n",
    "print(f\"  Max Iterations: {best_params_pca['max_iter']}\")\n",
    "print(f\"\\nBest Results:\")\n",
    "print(f\"  Worst Group Accuracy: {best_results_pca['worst_group_mean']:.3f}  {best_results_pca['worst_group_std']:.3f}\")\n",
    "print(f\"  Balanced Group Accuracy: {best_results_pca['balanced_group_mean']:.3f}  {best_results_pca['balanced_group_std']:.3f}\")\n",
    "print(f\"  Overall Accuracy: {best_results_pca['overall_mean']:.3f}  {best_results_pca['overall_std']:.3f}\")\n",
    "\n",
    "# Compare with best Logistic Regression results\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Best Logistic Regression Worst Group Acc: {best_results['worst_group_mean']:.3f}\")\n",
    "print(f\"Best PCA + Logistic Regression Worst Group Acc: {best_results_pca['worst_group_mean']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that the worst group accuracy is the same as with normal logistic regression, and the overall accuracy is just slightly lower. The variance of the worst group accuracy with PCA is slightly higher however. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to combine the logistic regression and the PCA logistic regression and see if the combined model will provide an even better overall accuracy and and an even higher worst group accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Ensemble Model (Logistic Regression + PCA Logistic Regression)\n",
      "================================================================================\n",
      "Ensemble Model Results:\n",
      "  Worst Group Accuracy: 0.251  0.141\n",
      "  Balanced Group Accuracy: 0.663  0.072\n",
      "  Overall Accuracy: 0.926  0.014\n",
      "\n",
      "Comparison with Individual Models:\n",
      "Best Logistic Regression:\n",
      "  Worst Group Accuracy: 0.304\n",
      "  Overall Accuracy: 0.928\n",
      "Best PCA + Logistic Regression:\n",
      "  Worst Group Accuracy: 0.282\n",
      "  Overall Accuracy: 0.917\n",
      "Ensemble Model:\n",
      "  Worst Group Accuracy: 0.251\n",
      "  Overall Accuracy: 0.926\n"
     ]
    }
   ],
   "source": [
    "class EnsembleModel():\n",
    "    \"\"\"\n",
    "    Ensemble model combining Logistic Regression and PCA + Logistic Regression.\n",
    "    Uses model averaging with the best hyperparameters found during tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=9):\n",
    "        self.random_state = random_state\n",
    "        # Set random seed for reproducibility\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        # Use best hyperparameters found during tuning\n",
    "        self.logistic_model = LogisticRegressionForCompetition(\n",
    "            learning_rate=0.1, \n",
    "            batch_size=20, \n",
    "            epochs=20\n",
    "        )\n",
    "        self.pca_model = PCALogisticRegression(\n",
    "            n_components=250, \n",
    "            learning_rate=0.001, \n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit both models on the training data.\"\"\"\n",
    "        # Set seed before fitting each model for reproducibility\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # Fit logistic regression model\n",
    "        self.logistic_model.fit(data)\n",
    "        \n",
    "        # Set seed again for second model\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state + 1)\n",
    "        \n",
    "        # Fit PCA + logistic regression model\n",
    "        self.pca_model.fit(data)\n",
    "    \n",
    "    def _get_logistic_probabilities(self, X):\n",
    "        \"\"\"Get probabilities from logistic regression model.\"\"\"\n",
    "        X_flat = self.logistic_model._flatten(X)\n",
    "        X_std = (X_flat - self.logistic_model.feature_mean) / self.logistic_model.feature_std\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.logistic_model.beta\n",
    "        return self.logistic_model._sigmoid(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using ensemble averaging.\"\"\"\n",
    "        # Get probabilities from both models\n",
    "        prob_logistic = self._get_logistic_probabilities(X)\n",
    "        prob_pca = self.pca_model.predict_proba(X)\n",
    "        \n",
    "        # Average the probabilities\n",
    "        avg_prob = (prob_logistic + prob_pca) / 2\n",
    "        \n",
    "        # Make final prediction based on averaged probabilities\n",
    "        ensemble_pred = (avg_prob > 0.5).astype(int)\n",
    "        \n",
    "        return ensemble_pred.reshape(-1, 1)\n",
    "\n",
    "# Evaluate the ensemble model with seeding\n",
    "print(\"Evaluating Ensemble Model (Logistic Regression + PCA Logistic Regression)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# see\n",
    "np.random.seed(9)\n",
    "ensemble_results = evaluate_model_with_subgroups(EnsembleModel, data_train, cv_folds=5, random_state=42)\n",
    "\n",
    "print(f\"Ensemble Model Results:\")\n",
    "print(f\"  Worst Group Accuracy: {ensemble_results['worst_group_mean']:.3f}  {ensemble_results['worst_group_std']:.3f}\")\n",
    "print(f\"  Balanced Group Accuracy: {ensemble_results['balanced_group_mean']:.3f}  {ensemble_results['balanced_group_std']:.3f}\")\n",
    "print(f\"  Overall Accuracy: {ensemble_results['overall_mean']:.3f}  {ensemble_results['overall_std']:.3f}\")\n",
    "\n",
    "print(f\"\\nComparison with Individual Models:\")\n",
    "print(f\"Best Logistic Regression:\")\n",
    "print(f\"  Worst Group Accuracy: {best_results['worst_group_mean']:.3f}\")\n",
    "print(f\"  Overall Accuracy: {best_results['overall_mean']:.3f}\")\n",
    "\n",
    "print(f\"Best PCA + Logistic Regression:\")\n",
    "print(f\"  Worst Group Accuracy: {best_results_pca['worst_group_mean']:.3f}\")\n",
    "print(f\"  Overall Accuracy: {best_results_pca['overall_mean']:.3f}\")\n",
    "\n",
    "print(f\"Ensemble Model:\")\n",
    "print(f\"  Worst Group Accuracy: {ensemble_results['worst_group_mean']:.3f}\")\n",
    "print(f\"  Overall Accuracy: {ensemble_results['overall_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two has led to a slightly lower worst group accuracy and slightly higher overall accuracy. \n",
    "Now a grid search will be performed when combining the two methods for an averaging model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Hyperparameter Tuning for Averaging Model\n",
      "============================================================\n",
      "Total combinations: 486\n",
      "Progress: 486/486 (100.0%) | Best worst group: 0.4244\n",
      "\n",
      "Best Enhanced Ensemble hyperparameters:\n",
      "  Logistic LR: 0.2\n",
      "  Logistic Batch Size: 10\n",
      "  Logistic Epochs: 15\n",
      "  PCA Components: 150\n",
      "  PCA LR: 0.005\n",
      "  PCA Max Iter: 1000\n",
      "\n",
      "Best Enhanced Ensemble Results:\n",
      "  Worst Group Accuracy: 0.4244  0.1362\n",
      "  Balanced Group Accuracy: 0.7277  0.0419\n",
      "  Overall Accuracy: 0.9190  0.0001\n"
     ]
    }
   ],
   "source": [
    "# Enhanced hyperparameter tuning for the averaging model (slightly expanded search space)\n",
    "print(\"Enhanced Hyperparameter Tuning for Averaging Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define slightly expanded hyperparameter ranges for both models\n",
    "logistic_learning_rates = [0.05, 0.1, 0.2]  # Increased from 2 to 3\n",
    "logistic_batch_sizes = [10, 20, 30]  # Increased from 2 to 3\n",
    "logistic_epochs = [15, 20, 25]  # Increased from 2 to 3\n",
    "\n",
    "pca_n_components = [150, 200, 250]  # Increased from 2 to 3\n",
    "pca_learning_rates = [0.001, 0.005, 0.01]  # Increased from 2 to 3\n",
    "pca_max_iters = [1000, 1500]  # Increased from 1 to 2\n",
    "\n",
    "best_ensemble_worst_group = 0\n",
    "best_ensemble_params = {}\n",
    "best_ensemble_results = {}\n",
    "\n",
    "total_combinations = len(logistic_learning_rates) * len(logistic_batch_sizes) * len(logistic_epochs) * len(pca_n_components) * len(pca_learning_rates) * len(pca_max_iters)\n",
    "current_combination = 0\n",
    "\n",
    "print(f\"Total combinations: {total_combinations}\")\n",
    "\n",
    "# Grid search through all combinations\n",
    "for log_lr in logistic_learning_rates:\n",
    "    for log_bs in logistic_batch_sizes:\n",
    "        for log_ep in logistic_epochs:\n",
    "            for pca_comp in pca_n_components:\n",
    "                for pca_lr in pca_learning_rates:\n",
    "                    for pca_iter in pca_max_iters:\n",
    "                        current_combination += 1\n",
    "                        \n",
    "                        # Progress indicator\n",
    "                        progress = (current_combination / total_combinations) * 100\n",
    "                        print(f\"\\rProgress: {current_combination}/{total_combinations} ({progress:.1f}%) | Best worst group: {best_ensemble_worst_group:.4f}\", end='', flush=True)\n",
    "                        \n",
    "                        # Create enhanced ensemble class with current hyperparameters\n",
    "                        class EnhancedEnsemble():\n",
    "                            def __init__(self):\n",
    "                                self.logistic_model = LogisticRegressionForCompetition(\n",
    "                                    learning_rate=log_lr, \n",
    "                                    batch_size=log_bs, \n",
    "                                    epochs=log_ep\n",
    "                                )\n",
    "                                self.pca_model = PCALogisticRegression(\n",
    "                                    n_components=pca_comp, \n",
    "                                    learning_rate=pca_lr, \n",
    "                                    max_iter=pca_iter\n",
    "                                )\n",
    "                                \n",
    "                            def fit(self, data):\n",
    "                                self.logistic_model.fit(data)\n",
    "                                self.pca_model.fit(data)\n",
    "                            \n",
    "                            def predict(self, X):\n",
    "                                # Get probabilities from logistic regression\n",
    "                                X_flat = self.logistic_model._flatten(X)\n",
    "                                X_std = (X_flat - self.logistic_model.feature_mean) / self.logistic_model.feature_std\n",
    "                                X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "                                z = X_aug @ self.logistic_model.beta\n",
    "                                prob_logistic = self.logistic_model._sigmoid(z)\n",
    "                                \n",
    "                                # Get probabilities from PCA model\n",
    "                                prob_pca = self.pca_model.predict_proba(X)\n",
    "                                \n",
    "                                # Average the probabilities\n",
    "                                avg_prob = (prob_logistic + prob_pca) / 2\n",
    "                                ensemble_pred = (avg_prob > 0.5).astype(int)\n",
    "                                \n",
    "                                return ensemble_pred.reshape(-1, 1)\n",
    "                        \n",
    "                        try:\n",
    "                            # Evaluate current hyperparameter combination\n",
    "                            results = evaluate_model_with_subgroups(\n",
    "                                EnhancedEnsemble, \n",
    "                                data_train, \n",
    "                                cv_folds=3  # Keep folds at 3 for faster computation\n",
    "                            )\n",
    "                            \n",
    "                            # Check if this is the best worst group accuracy\n",
    "                            if results['worst_group_mean'] > best_ensemble_worst_group:\n",
    "                                best_ensemble_worst_group = results['worst_group_mean']\n",
    "                                best_ensemble_params = {\n",
    "                                    'logistic_lr': log_lr,\n",
    "                                    'logistic_bs': log_bs, \n",
    "                                    'logistic_epochs': log_ep,\n",
    "                                    'pca_components': pca_comp,\n",
    "                                    'pca_lr': pca_lr,\n",
    "                                    'pca_max_iter': pca_iter\n",
    "                                }\n",
    "                                best_ensemble_results = results\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "\n",
    "print(f\"\\n\\nBest Enhanced Ensemble hyperparameters:\")\n",
    "print(f\"  Logistic LR: {best_ensemble_params['logistic_lr']}\")\n",
    "print(f\"  Logistic Batch Size: {best_ensemble_params['logistic_bs']}\")\n",
    "print(f\"  Logistic Epochs: {best_ensemble_params['logistic_epochs']}\")\n",
    "print(f\"  PCA Components: {best_ensemble_params['pca_components']}\")\n",
    "print(f\"  PCA LR: {best_ensemble_params['pca_lr']}\")\n",
    "print(f\"  PCA Max Iter: {best_ensemble_params['pca_max_iter']}\")\n",
    "\n",
    "print(f\"\\nBest Enhanced Ensemble Results:\")\n",
    "print(f\"  Worst Group Accuracy: {best_ensemble_results['worst_group_mean']:.4f}  {best_ensemble_results['worst_group_std']:.4f}\")\n",
    "print(f\"  Balanced Group Accuracy: {best_ensemble_results['balanced_group_mean']:.4f}  {best_ensemble_results['balanced_group_std']:.4f}\")\n",
    "print(f\"  Overall Accuracy: {best_ensemble_results['overall_mean']:.4f}  {best_ensemble_results['overall_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a grid search between logistic regression and PCA logistic regression, it found a high worst group accuracy of 42,44% and and overall accuracy of 91,9%, with a very low variance. Therefore this model will be chosen as the best model, having the following parameters: \n",
    "\n",
    " Logistic LR: 0.2\n",
    "\n",
    "  Logistic Batch Size: 10\n",
    "  \n",
    "  Logistic Epochs: 15\n",
    "  \n",
    "  PCA Components: 150\n",
    "  \n",
    "  PCA LR: 0.005\n",
    "  \n",
    "  PCA Max Iter: 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model\n",
    "Use the cell below to define your best model, in the form of a class called `Model`. The class must have (at least) the following two methods:\n",
    "* `fit`: trains the model based on an instance of the class `Data` that is used as input argument.\n",
    "* `predict`: predicts the target variable for observations with input features `X`, that is used as input argument.\n",
    "\n",
    "For the sake of clarity, we have defined a placeholder version of the class `Model`. This placeholder version can be replaced by your best model class `Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \"\"\"\n",
    "    Ensemble model combining Logistic Regression and PCA + Logistic Regression.\n",
    "    Uses model averaging with optimized hyperparameters found during tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        # Use best hyperparameters found during tuning\n",
    "        self.logistic_model = LogisticRegressionForCompetition(\n",
    "            learning_rate=0.2, \n",
    "            batch_size=10, \n",
    "            epochs=15\n",
    "        )\n",
    "        self.pca_model = PCALogisticRegression(\n",
    "            n_components=150, \n",
    "            learning_rate=0.005, \n",
    "            max_iter=1000\n",
    "        )\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit both models on the training data.\"\"\"\n",
    "        # Fit logistic regression model\n",
    "        self.logistic_model.fit(data)\n",
    "        \n",
    "        # Fit PCA + logistic regression model\n",
    "        self.pca_model.fit(data)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using ensemble averaging.\"\"\"\n",
    "        # Get probabilities from logistic regression model\n",
    "        X_flat = self.logistic_model._flatten(X)\n",
    "        X_std = (X_flat - self.logistic_model.feature_mean) / self.logistic_model.feature_std\n",
    "        X_aug = np.hstack([np.ones((X_std.shape[0], 1)), X_std])\n",
    "        z = X_aug @ self.logistic_model.beta\n",
    "        prob_logistic = self.logistic_model._sigmoid(z)\n",
    "        \n",
    "        # Get probabilities from PCA model\n",
    "        prob_pca = self.pca_model.predict_proba(X)\n",
    "        \n",
    "        # Average the probabilities\n",
    "        avg_prob = (prob_logistic + prob_pca) / 2\n",
    "        \n",
    "        # Make final prediction based on averaged probabilities\n",
    "        ensemble_pred = (avg_prob > 0.5).astype(int)\n",
    "        \n",
    "        return ensemble_pred.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization performance\n",
    "The cell below computes the generalization performance of `Model` and should **NOT** be changed.\n",
    "\n",
    "Note that the accuracy is computed twice: once on the whole test set at once, and a second time on the individual observations one-by-one. The results should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (default):    0.518\n",
      "Accuracy (one-by-one): 0.518\n"
     ]
    }
   ],
   "source": [
    "### DO NOT CHANGE THIS CELL. IT WILL BE USED TO TEST YOUR CUSTOM MODEL.\n",
    "### CHANGING THIS CELL WILL RESULT IN POINT DEDUCTIONS.\n",
    "\n",
    "# Load data\n",
    "with open('data/data_train.pkl', 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "try:\n",
    "    with open('data/data_test.pkl', 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    with open('data/data_test_FAKE.pkl', 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "\n",
    "# Custom model parameters\n",
    "try:\n",
    "    model_kwargs\n",
    "except NameError:\n",
    "    model_kwargs = {}\n",
    "\n",
    "# Train custom model\n",
    "model = Model(**model_kwargs)\n",
    "model.fit(data_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(data_test.X)\n",
    "print(\"Accuracy (default):   \", np.mean(y_pred == data_test.y))\n",
    "\n",
    "# Predict on test set (custom implementation predicting one-by-one)\n",
    "errors = []\n",
    "for i in range(data_test.X.shape[0]):\n",
    "    y_pred = model.predict(data_test.X[i].reshape(1, -1))\n",
    "    errors.append(y_pred == data_test.y[i])\n",
    "print(\"Accuracy (one-by-one):\", np.mean(errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
